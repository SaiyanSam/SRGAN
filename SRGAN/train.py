{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b03123-3871-44cf-80b8-dac42fc35cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TL_BACKEND'] = 'tensorflow' # Just modify this line, easily switch to any framework! PyTorch will coming soon!\n",
    "# os.environ['TL_BACKEND'] = 'mindspore'\n",
    "# os.environ['TL_BACKEND'] = 'paddle'\n",
    "# os.environ['TL_BACKEND'] = 'torch'\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorlayerx as tlx\n",
    "from tensorlayerx.dataflow import Dataset, DataLoader\n",
    "from srgan import SRGAN_g, SRGAN_d\n",
    "from config import config\n",
    "from tensorlayerx.vision.transforms import Compose, RandomCrop, Normalize, RandomFlipHorizontal, Resize, HWC2CHW\n",
    "import vgg\n",
    "from tensorlayerx.model import TrainOneStep\n",
    "from tensorlayerx.nn import Module\n",
    "import cv2\n",
    "tlx.set_device('GPU')\n",
    "\n",
    "###====================== HYPER-PARAMETERS ===========================###\n",
    "batch_size = 8\n",
    "n_epoch_init = config.TRAIN.n_epoch_init\n",
    "n_epoch = config.TRAIN.n_epoch\n",
    "# create folders to save result images and trained models\n",
    "save_dir = \"samples\"\n",
    "tlx.files.exists_or_mkdir(save_dir)\n",
    "checkpoint_dir = \"models\"\n",
    "tlx.files.exists_or_mkdir(checkpoint_dir)\n",
    "\n",
    "hr_transform = Compose([\n",
    "    RandomCrop(size=(384, 384)),\n",
    "    RandomFlipHorizontal(),\n",
    "])\n",
    "nor = Compose([Normalize(mean=(127.5), std=(127.5), data_format='HWC'),\n",
    "              HWC2CHW()])\n",
    "lr_transform = Resize(size=(96, 96))\n",
    "\n",
    "train_hr_imgs = tlx.vision.load_images(path=config.TRAIN.hr_img_path, n_threads = 32)\n",
    "\n",
    "class TrainData(Dataset):\n",
    "\n",
    "    def __init__(self, hr_trans=hr_transform, lr_trans=lr_transform):\n",
    "        self.train_hr_imgs = train_hr_imgs\n",
    "        self.hr_trans = hr_trans\n",
    "        self.lr_trans = lr_trans\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.train_hr_imgs[index]\n",
    "        hr_patch = self.hr_trans(img)\n",
    "        lr_patch = self.lr_trans(hr_patch)\n",
    "        return nor(lr_patch), nor(hr_patch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_hr_imgs)\n",
    "\n",
    "\n",
    "class WithLoss_init(Module):\n",
    "    def __init__(self, G_net, loss_fn):\n",
    "        super(WithLoss_init, self).__init__()\n",
    "        self.net = G_net\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, lr, hr):\n",
    "        out = self.net(lr)\n",
    "        loss = self.loss_fn(out, hr)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class WithLoss_D(Module):\n",
    "    def __init__(self, D_net, G_net, loss_fn):\n",
    "        super(WithLoss_D, self).__init__()\n",
    "        self.D_net = D_net\n",
    "        self.G_net = G_net\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, lr, hr):\n",
    "        fake_patchs = self.G_net(lr)\n",
    "        logits_fake = self.D_net(fake_patchs)\n",
    "        logits_real = self.D_net(hr)\n",
    "        d_loss1 = self.loss_fn(logits_real, tlx.ones_like(logits_real))\n",
    "        d_loss1 = tlx.ops.reduce_mean(d_loss1)\n",
    "        d_loss2 = self.loss_fn(logits_fake, tlx.zeros_like(logits_fake))\n",
    "        d_loss2 = tlx.ops.reduce_mean(d_loss2)\n",
    "        d_loss = d_loss1 + d_loss2\n",
    "        return d_loss\n",
    "\n",
    "\n",
    "class WithLoss_G(Module):\n",
    "    def __init__(self, D_net, G_net, vgg, loss_fn1, loss_fn2):\n",
    "        super(WithLoss_G, self).__init__()\n",
    "        self.D_net = D_net\n",
    "        self.G_net = G_net\n",
    "        self.vgg = vgg\n",
    "        self.loss_fn1 = loss_fn1\n",
    "        self.loss_fn2 = loss_fn2\n",
    "\n",
    "    def forward(self, lr, hr):\n",
    "        fake_patchs = self.G_net(lr)\n",
    "        logits_fake = self.D_net(fake_patchs)\n",
    "        feature_fake = self.vgg((fake_patchs + 1) / 2.)\n",
    "        feature_real = self.vgg((hr + 1) / 2.)\n",
    "        g_gan_loss = 1e-3 * self.loss_fn1(logits_fake, tlx.ones_like(logits_fake))\n",
    "        g_gan_loss = tlx.ops.reduce_mean(g_gan_loss)\n",
    "        mse_loss = self.loss_fn2(fake_patchs, hr)\n",
    "        vgg_loss = 2e-6 * self.loss_fn2(feature_fake, feature_real)\n",
    "        g_loss = mse_loss + vgg_loss + g_gan_loss\n",
    "        return g_loss\n",
    "\n",
    "\n",
    "G = SRGAN_g()\n",
    "D = SRGAN_d()\n",
    "VGG = vgg.VGG19(pretrained=True, end_with='pool4', mode='dynamic')\n",
    "# automatic init layers weights shape with input tensor.\n",
    "# Calculating and filling 'in_channels' of each layer is a very troublesome thing.\n",
    "# So, just use 'init_build' with input shape. 'in_channels' of each layer will be automaticlly set.\n",
    "G.init_build(tlx.nn.Input(shape=(8, 3, 96, 96)))\n",
    "D.init_build(tlx.nn.Input(shape=(8, 3, 384, 384)))\n",
    "\n",
    "\n",
    "def train():\n",
    "    G.set_train()\n",
    "    D.set_train()\n",
    "    VGG.set_eval()\n",
    "    train_ds = TrainData()\n",
    "    train_ds_img_nums = len(train_ds)\n",
    "    train_ds = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    lr_v = tlx.optimizers.lr.StepDecay(learning_rate=0.05, step_size=1000, gamma=0.1, last_epoch=-1, verbose=True)\n",
    "    g_optimizer_init = tlx.optimizers.Momentum(lr_v, 0.9)\n",
    "    g_optimizer = tlx.optimizers.Momentum(lr_v, 0.9)\n",
    "    d_optimizer = tlx.optimizers.Momentum(lr_v, 0.9)\n",
    "    g_weights = G.trainable_weights\n",
    "    d_weights = D.trainable_weights\n",
    "    net_with_loss_init = WithLoss_init(G, loss_fn=tlx.losses.mean_squared_error)\n",
    "    net_with_loss_D = WithLoss_D(D_net=D, G_net=G, loss_fn=tlx.losses.sigmoid_cross_entropy)\n",
    "    net_with_loss_G = WithLoss_G(D_net=D, G_net=G, vgg=VGG, loss_fn1=tlx.losses.sigmoid_cross_entropy,\n",
    "                                 loss_fn2=tlx.losses.mean_squared_error)\n",
    "\n",
    "    trainforinit = TrainOneStep(net_with_loss_init, optimizer=g_optimizer_init, train_weights=g_weights)\n",
    "    trainforG = TrainOneStep(net_with_loss_G, optimizer=g_optimizer, train_weights=g_weights)\n",
    "    trainforD = TrainOneStep(net_with_loss_D, optimizer=d_optimizer, train_weights=d_weights)\n",
    "\n",
    "    # initialize learning (G)\n",
    "    n_step_epoch = round(train_ds_img_nums // batch_size)\n",
    "    for epoch in range(n_epoch_init):\n",
    "        for step, (lr_patch, hr_patch) in enumerate(train_ds):\n",
    "            step_time = time.time()\n",
    "            loss = trainforinit(lr_patch, hr_patch)\n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n",
    "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, float(loss)))\n",
    "\n",
    "    # adversarial learning (G, D)\n",
    "    n_step_epoch = round(train_ds_img_nums // batch_size)\n",
    "    for epoch in range(n_epoch):\n",
    "        for step, (lr_patch, hr_patch) in enumerate(train_ds):\n",
    "            step_time = time.time()\n",
    "            loss_g = trainforG(lr_patch, hr_patch)\n",
    "            loss_d = trainforD(lr_patch, hr_patch)\n",
    "            print(\n",
    "                \"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss:{:.3f}, d_loss: {:.3f}\".format(\n",
    "                    epoch, n_epoch, step, n_step_epoch, time.time() - step_time, float(loss_g), float(loss_d)))\n",
    "        # dynamic learning rate update\n",
    "        lr_v.step()\n",
    "\n",
    "        if (epoch != 0) and (epoch % 10 == 0):\n",
    "            G.save_weights(os.path.join(checkpoint_dir, 'g.npz'), format='npz_dict')\n",
    "            D.save_weights(os.path.join(checkpoint_dir, 'd.npz'), format='npz_dict')\n",
    "\n",
    "def evaluate():\n",
    "    ###====================== PRE-LOAD DATA ===========================###\n",
    "    valid_hr_imgs = tlx.vision.load_images(path=config.VALID.hr_img_path )\n",
    "    ###========================LOAD WEIGHTS ============================###\n",
    "    G.load_weights(os.path.join(checkpoint_dir, 'g.npz'), format='npz_dict')\n",
    "    G.set_eval()\n",
    "    imid = 0  # 0: 企鹅  81: 蝴蝶 53: 鸟  64: 古堡\n",
    "    valid_hr_img = valid_hr_imgs[imid]\n",
    "    valid_lr_img = np.asarray(valid_hr_img)\n",
    "    hr_size1 = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n",
    "    valid_lr_img = cv2.resize(valid_lr_img, dsize=(hr_size1[1] // 4, hr_size1[0] // 4))\n",
    "    valid_lr_img_tensor = (valid_lr_img / 127.5) - 1  # rescale to ［－1, 1]\n",
    "\n",
    "\n",
    "    valid_lr_img_tensor = np.asarray(valid_lr_img_tensor, dtype=np.float32)\n",
    "    valid_lr_img_tensor = np.transpose(valid_lr_img_tensor,axes=[2, 0, 1])\n",
    "    valid_lr_img_tensor = valid_lr_img_tensor[np.newaxis, :, :, :]\n",
    "    valid_lr_img_tensor= tlx.ops.convert_to_tensor(valid_lr_img_tensor)\n",
    "    size = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n",
    "\n",
    "    out = tlx.ops.convert_to_numpy(G(valid_lr_img_tensor))\n",
    "    out = np.asarray((out + 1) * 127.5, dtype=np.uint8)\n",
    "    out = np.transpose(out[0], axes=[1, 2, 0])\n",
    "    print(\"LR size: %s /  generated HR size: %s\" % (size, out.shape))  # LR size: (339, 510, 3) /  gen HR size: (1, 1356, 2040, 3)\n",
    "    print(\"[*] save images\")\n",
    "    tlx.vision.save_image(out, file_name='valid_gen.png', path=save_dir)\n",
    "    tlx.vision.save_image(valid_lr_img, file_name='valid_lr.png', path=save_dir)\n",
    "    tlx.vision.save_image(valid_hr_img, file_name='valid_hr.png', path=save_dir)\n",
    "    out_bicu = cv2.resize(valid_lr_img, dsize = [size[1] * 4, size[0] * 4], interpolation = cv2.INTER_CUBIC)\n",
    "    tlx.vision.save_image(out_bicu, file_name='valid_hr_cubic.png', path=save_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--mode', type=str, default='train', help='train, eval')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    tlx.global_flag['mode'] = args.mode\n",
    "\n",
    "    if tlx.global_flag['mode'] == 'train':\n",
    "        train()\n",
    "    elif tlx.global_flag['mode'] == 'eval':\n",
    "        evaluate()\n",
    "    else:\n",
    "        raise Exception(\"Unknow --mode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
